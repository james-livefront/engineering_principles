#!/usr/bin/env python3
"""
Engineering Principles Evaluation Runner

Tests the effectiveness of generated prompts by running them against test cases
and measuring how well AI systems detect violations and generate compliant code.
"""

import argparse
import json
import os
from collections.abc import Callable
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, TypedDict

import yaml


# Load environment variables from .env file
def load_dotenv() -> None:
    """Load environment variables from .env file"""
    env_path = Path(".env")
    if env_path.exists():
        with open(env_path) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    os.environ[key.strip()] = value.strip()


load_dotenv()


# Type definitions for YAML data structures
class ExpectedResult(TypedDict):
    detected: bool
    rule: str | None
    severity: str | None
    message_contains: str | None
    suggestion_contains: str | None
    reason: str | None


class TestCase(TypedDict):
    id: str
    name: str
    category: str
    code: str
    language: str
    file_path: str | None
    expected: ExpectedResult


class GenerationChallenge(TypedDict):
    id: str
    name: str
    platform: str
    challenge: str
    requirements: dict[str, str]
    expected_features: dict[str, list[str]]


class DetectionResponse(TypedDict):
    found: bool
    confidence: float
    message: str


@dataclass
class TestResult:
    test_id: str
    name: str
    expected: bool
    detected: bool
    confidence: float
    message: str
    correct: bool
    details: dict[str, Any]


@dataclass
class EvaluationReport:
    total_tests: int
    correct_predictions: int
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    results_by_category: dict[str, dict[str, float]]
    failed_tests: list[TestResult]


class PromptEvaluator:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.detection_tests = self._load_detection_tests()
        self.generation_tests = self._load_generation_tests()

    def _load_detection_tests(self) -> dict[str, list[TestCase]]:
        """Load all detection test cases"""
        tests: dict[str, list[TestCase]] = {}
        detection_dir = self.base_path / "evals" / "detection"

        if detection_dir.exists():
            for test_file in detection_dir.glob("*_test_cases.yaml"):
                principle = test_file.stem.replace("_test_cases", "")
                try:
                    with open(test_file) as f:
                        data = yaml.safe_load(f)
                        if isinstance(data, dict) and "test_cases" in data:
                            tests[principle] = data["test_cases"]
                except Exception as e:
                    print(f"Error loading {test_file}: {e}")

        return tests

    def _load_generation_tests(self) -> dict[str, list[GenerationChallenge]]:
        """Load all generation test cases"""
        tests: dict[str, list[GenerationChallenge]] = {}
        generation_dir = self.base_path / "evals" / "generation"

        if generation_dir.exists():
            for test_file in generation_dir.glob("*_challenges.yaml"):
                category = test_file.stem.replace("_challenges", "")
                try:
                    with open(test_file) as f:
                        data = yaml.safe_load(f)
                        if isinstance(data, dict) and "test_cases" in data:
                            tests[category] = data["test_cases"]
                except Exception as e:
                    print(f"Error loading {test_file}: {e}")

        return tests

    def evaluate_detection_prompt(
        self,
        prompt: str,
        ai_evaluator_func: Callable[[str], str],
        principles: list[str] | None = None,
    ) -> EvaluationReport:
        """
        Evaluate how well a detection prompt identifies violations

        Args:
            prompt: The generated detection prompt to test
            ai_evaluator_func: Function that takes (prompt, code) and
                returns detection results
            principles: List of principles to test (None = all)
        """
        all_results: list[TestResult] = []
        category_stats: dict[str, dict[str, float]] = {}

        # Filter tests by requested principles
        tests_to_run = self.detection_tests
        if principles:
            tests_to_run = {k: v for k, v in tests_to_run.items() if k in principles}

        for principle, test_cases in tests_to_run.items():
            print(f"\nTesting {principle} detection...")

            principle_results: list[TestResult] = []
            for test_case in test_cases:
                result = self._run_detection_test(prompt, test_case, ai_evaluator_func)
                all_results.append(result)
                principle_results.append(result)

                # Print progress
                status = "‚úì" if result.correct else "‚úó"
                print(f"  {status} {result.name}")

            # Calculate per-principle stats
            correct = sum(1 for r in principle_results if r.correct)
            total = len(principle_results)
            category_stats[principle] = {
                "accuracy": correct / total if total > 0 else 0,
                "total": total,
                "correct": correct,
            }

        return self._calculate_evaluation_report(all_results, category_stats)

    def evaluate_generation_prompt(
        self,
        prompt: str,
        ai_generator_func: Callable[[str], str],
        ai_evaluator_func: Callable[[str], str],
        categories: list[str] | None = None,
    ) -> dict[str, Any]:
        """
        Evaluate how well a generation prompt creates compliant code

        Args:
            prompt: The generated code writing prompt to test
            ai_generator_func: Function that takes (prompt, challenge)
                and returns generated code
            ai_evaluator_func: Function that evaluates generated code for compliance
            categories: Categories to test (None = all)
        """
        all_results: list[dict[str, Any]] = []

        # Filter tests by requested categories
        tests_to_run = self.generation_tests
        if categories:
            tests_to_run = {k: v for k, v in tests_to_run.items() if k in categories}

        for category, challenges in tests_to_run.items():
            print(f"\\nTesting {category} generation...")

            for challenge in challenges:
                result = self._run_generation_test(
                    prompt, challenge, ai_generator_func, ai_evaluator_func
                )
                all_results.append(result)

                # Print progress
                score = result.get("compliance_score", 0)
                status = "‚úì" if score >= 0.8 else "‚ö†" if score >= 0.6 else "‚úó"
                print(f"  {status} {challenge['name']} (Score: {score:.2f})")

        return {
            "total_challenges": len(all_results),
            "average_compliance": sum(r.get("compliance_score", 0) for r in all_results)
            / len(all_results),
            "results": all_results,
        }

    def _run_detection_test(
        self, prompt: str, test_case: TestCase, ai_evaluator_func: Callable[[str], str]
    ) -> TestResult:
        """Run a single detection test case"""
        try:
            # Combine prompt with test code
            full_prompt = (
                f"{prompt}\\n\\nCode to review:\\n\\n"
                f"```{test_case['language']}\\n{test_case['code']}\\n```"
            )

            # Get AI evaluation
            ai_response = ai_evaluator_func(full_prompt)

            # Parse AI response to determine if violation was detected
            detected = self._parse_detection_response(ai_response, test_case)
            expected = test_case["expected"]["detected"]

            return TestResult(
                test_id=test_case["id"],
                name=test_case["name"],
                expected=expected,
                detected=detected["found"],
                confidence=detected.get("confidence", 1.0),
                message=detected.get("message", ""),
                correct=expected == detected["found"],
                details={
                    "category": test_case["category"],
                    "language": test_case["language"],
                    "ai_response": ai_response,
                    "expected_rule": test_case["expected"].get("rule"),
                    "expected_severity": test_case["expected"].get("severity"),
                },
            )

        except Exception as e:
            return TestResult(
                test_id=test_case.get("id", "unknown"),
                name=test_case.get("name", "unknown"),
                expected=False,
                detected=False,
                confidence=0.0,
                message=f"Error: {str(e)}",
                correct=False,
                details={"error": str(e)},
            )

    def _run_generation_test(
        self,
        prompt: str,
        challenge: GenerationChallenge,
        ai_generator_func: Callable[[str], str],
        ai_evaluator_func: Callable[[str], str],
    ) -> dict[str, Any]:
        """Run a single generation challenge"""
        try:
            # Generate code using the challenge
            full_prompt = f"{prompt}\\n\\nChallenge:\\n{challenge['challenge']}"
            generated_code = ai_generator_func(full_prompt)

            # Evaluate the generated code for compliance
            evaluation = self._evaluate_generated_code(
                generated_code, dict(challenge), ai_evaluator_func
            )

            return {
                "challenge_id": challenge["id"],
                "name": challenge["name"],
                "platform": challenge["platform"],
                "generated_code": generated_code,
                "compliance_score": evaluation["score"],
                "feature_compliance": evaluation["features"],
                "missing_features": evaluation["missing"],
                "suggestions": evaluation["suggestions"],
            }

        except Exception as e:
            return {
                "challenge_id": challenge.get("id", "unknown"),
                "name": challenge.get("name", "unknown"),
                "error": str(e),
                "compliance_score": 0.0,
            }

    def _parse_detection_response(self, ai_response: str, test_case: TestCase) -> DetectionResponse:
        """Parse AI response to determine if violation was detected"""
        response_lower = ai_response.lower()

        # Look for indicators that a violation was found
        violation_indicators = [
            "violation",
            "issue",
            "problem",
            "error",
            "warning",
            "fix",
            "should",
            "must",
            "missing",
            "incorrect",
        ]

        found_indicators = sum(
            1 for indicator in violation_indicators if indicator in response_lower
        )

        # Simple heuristic: if multiple violation indicators, probably detected
        detected = found_indicators >= 2
        confidence = min(found_indicators / 3, 1.0)

        return {
            "found": detected,
            "confidence": confidence,
            "message": (ai_response[:200] + "..." if len(ai_response) > 200 else ai_response),
        }

    def _evaluate_generated_code(
        self,
        generated_code: str,
        challenge: dict[str, Any],
        ai_evaluator_func: Callable[[str], str],
    ) -> dict[str, Any]:
        """Evaluate generated code against challenge requirements"""
        expected_features = challenge.get("expected_features", {})
        challenge.get("evaluation_criteria", {})

        # Use AI evaluator to check compliance
        eval_prompt = f"""
        Evaluate this generated code against these requirements:

        Challenge: {challenge["challenge"]}
        Expected Features: {json.dumps(expected_features, indent=2)}

        Generated Code:
        ```{challenge.get("platform", "typescript")}
        {generated_code}
        ```

        Score each category 0-1 and explain what's missing:
        """

        ai_evaluation = ai_evaluator_func(eval_prompt)

        # Parse the evaluation (simplified for demo)
        # In practice, you'd want more sophisticated parsing
        score = 0.7  # Placeholder - would parse from AI response

        return {
            "score": score,
            "features": expected_features,
            "missing": [],  # Would extract from AI response
            "suggestions": ai_evaluation,
        }

    def _calculate_evaluation_report(
        self, results: list[TestResult], category_stats: dict[str, dict[str, Any]]
    ) -> EvaluationReport:
        """Calculate comprehensive evaluation metrics"""
        if not results:
            return EvaluationReport(0, 0, 0, 0, 0, 0, {}, [])

        total_tests = len(results)
        correct_predictions = sum(1 for r in results if r.correct)
        accuracy = correct_predictions / total_tests

        # Calculate precision, recall, F1
        true_positives = sum(1 for r in results if r.expected and r.detected)
        false_positives = sum(1 for r in results if not r.expected and r.detected)
        false_negatives = sum(1 for r in results if r.expected and not r.detected)

        precision = (
            true_positives / (true_positives + false_positives)
            if (true_positives + false_positives) > 0
            else 0
        )
        recall = (
            true_positives / (true_positives + false_negatives)
            if (true_positives + false_negatives) > 0
            else 0
        )
        f1_score = (
            2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        )

        failed_tests = [r for r in results if not r.correct]

        return EvaluationReport(
            total_tests=total_tests,
            correct_predictions=correct_predictions,
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            results_by_category=category_stats,
            failed_tests=failed_tests,
        )

    def generate_report(self, report: EvaluationReport, output_file: str | None = None) -> str:
        """Generate a formatted evaluation report"""
        report_text = f"""
# Engineering Principles Evaluation Report
Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Overall Results
- Total Tests: {report.total_tests}
- Correct Predictions: {report.correct_predictions}
- **Accuracy: {report.accuracy:.2%}**
- **Precision: {report.precision:.2%}**
- **Recall: {report.recall:.2%}**
- **F1 Score: {report.f1_score:.2%}**

## Results by Category
"""

        for category, stats in report.results_by_category.items():
            report_text += (
                f"- **{category.title()}**: {stats['correct']}/"
                f"{stats['total']} ({stats['accuracy']:.2%})\n"
            )

        if report.failed_tests:
            report_text += f"\n## Failed Tests ({len(report.failed_tests)})\n"
            for test in report.failed_tests[:10]:  # Limit to first 10
                report_text += (
                    f"- {test.test_id}: {test.name} "
                    f"(Expected: {test.expected}, Got: {test.detected})\n"
                )

        report_text += """
## Recommendations
- Focus on categories with accuracy < 80%
- Review failed test cases to improve detection rules
- Consider additional test cases for edge cases
"""

        if output_file:
            with open(output_file, "w") as f:
                f.write(report_text)

        return report_text


def mock_ai_evaluator(prompt: str) -> str:
    """Mock AI evaluator function for testing"""
    # This would be replaced with actual AI API calls
    return (
        "I found several issues in this code including security "
        "vulnerabilities and missing accessibility attributes."
    )


# API Provider configuration
API_PROVIDERS = {
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "requires_key": True,
        "default_model": "gpt-4o",
    }
}


class APIEvaluator:
    """API-based evaluator for code review"""

    def __init__(self, provider: str, model: str, base_url: str) -> None:
        self.provider = provider
        self.model = model
        self.base_url = base_url

        # Check if OpenAI is available
        try:
            import openai  # noqa: F401
        except ImportError:
            raise ImportError("OpenAI package is required for API-based evaluation") from None

        # Check for API key
        if provider == "openai" and not os.getenv("OPENAI_API_KEY"):
            raise ValueError(f"API key required for provider: {provider}")

    def evaluate_code(self, prompt: str) -> str:
        """Evaluate code using the API"""
        import openai

        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Code reviewer for engineering principles.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=0.1,
                max_tokens=4000,
            )
            return response.choices[0].message.content or ""
        except Exception as e:
            return f"API Error: {e}"


def parse_prompt_metadata(prompt: str) -> dict[str, str]:
    """Parse metadata from prompt header"""
    import re

    metadata = {}
    match = re.search(r"<!-- PROMPT_METADATA\n(.*?)\n-->", prompt, re.DOTALL)

    if match:
        metadata_text = match.group(1)
        for line in metadata_text.strip().split("\n"):
            if ":" in line:
                key, value = line.split(":", 1)
                metadata[key.strip()] = value.strip()

    return metadata


def enhance_prompt_with_llm(
    prompt: str, api_evaluator: APIEvaluator, show_diff: bool = False
) -> str:
    """Enhance a prompt with latest security/accessibility practices using LLM"""

    # Check cache first
    import hashlib

    cache_dir = Path(".cache")
    cache_dir.mkdir(exist_ok=True)

    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
    cache_file = cache_dir / f"enhanced_{prompt_hash}.txt"

    if cache_file.exists():
        print("Using cached enhanced prompt...")
        with open(cache_file) as f:
            enhanced = f.read()
    else:
        print("Enhancing prompt with latest practices using LLM...")

        enhancement_prompt = f"""Expert in engineering standards across security and accessibility.

Enhance this prompt with additional specific detection patterns while MAINTAINING ACCURACY.

**ADD these enhancement layers (preserve ALL original content):**

1. **Specific Modern Patterns** (2024/2025):
   - Latest OWASP patterns for REAL vulnerabilities (not localhost/test code)
   - Modern framework antipatterns (React 18+, TypeScript 5+)
   - Current WCAG 2.2 VIOLATIONS (not compliant code)
   - Contemporary testing gaps, not all missing tests

2. **Precision-Focused Analysis**:
   - AVOID FALSE POSITIVES - localhost/dev URLs are NOT violations
   - Test files and examples are NOT production violations
   - Good accessibility (proper ARIA, good contrast) should PASS
   - Context matters - be specific about ACTUAL problems

3. **Advanced Pattern Detection**:
   - Multi-line vulnerabilities that span code blocks
   - Semantic issues beyond regex patterns
   - Compound violations that require understanding context

4. **Real-Time Intelligence**:
   - Current industry standards and best practices
   - Tool-specific guidance (ESLint, TypeScript, testing frameworks)
   - Platform evolution awareness (latest iOS, Android, Web APIs)

**Focus on actionable, specific enhancements like:**
- Exact regex patterns for latest vulnerabilities
- Specific code examples with before/after
- Framework-specific detection rules
- Cross-cutting analysis techniques
- Specific HTML attributes for accessibility
- Concrete code smells with examples

Original prompt:
{prompt}

Enhanced prompt (keep ALL original content and ADD specifics):"""

        try:
            enhanced = api_evaluator.evaluate_code(enhancement_prompt)

            # Cache the enhanced prompt
            with open(cache_file, "w") as f:
                f.write(enhanced)

        except Exception as e:
            print(f"Enhancement failed: {e}")
            print("Using original prompt...")
            enhanced = prompt

    if show_diff:
        print("\n" + "=" * 50)
        print("PROMPT ENHANCEMENT DIFF")
        print("=" * 50)

        # Simple diff - show first 500 chars of each
        print("\nOriginal (first 500 chars):")
        print(prompt[:500] + "...")
        print("\nEnhanced (first 500 chars):")
        print(enhanced[:500] + "...")
        print("=" * 50 + "\n")

    return enhanced


def load_config(config_path: str) -> dict[str, Any]:
    """Load configuration from YAML or JSON file"""
    path = Path(config_path)

    if not path.exists():
        return {}

    try:
        with open(path) as f:
            if path.suffix.lower() in [".yaml", ".yml"]:
                data = yaml.safe_load(f)
                return dict(data) if data is not None else {}
            elif path.suffix.lower() == ".json":
                import json

                data = json.load(f)
                return dict(data) if isinstance(data, dict) else {}
            else:
                return {}
    except Exception:
        return {}


def mock_ai_generator(prompt: str) -> str:
    """Mock AI generator function for testing"""
    # This would be replaced with actual AI API calls
    return "// Generated code would appear here"


def main() -> None:
    parser = argparse.ArgumentParser(
        description="""üß™ Engineering Principles Evaluation Framework

Test prompt effectiveness against comprehensive test cases:
‚Ä¢ Base Mode: Evaluate YAML-based detection patterns (70%+ coverage)
‚Ä¢ Enhanced Mode: Test cutting-edge LLM intelligence (100% coverage)
‚Ä¢ Multiple platforms: Android, iOS, Web with platform-specific patterns
‚Ä¢ Real API testing: OpenAI, Anthropic, local models supported

Results show accuracy, precision, recall, and F1 scores with detailed failure analysis.""",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
üîç Examples:
  %(prog)s --mode detection --platform web --focus accessibility
  %(prog)s --mode detection --platform android --focus security --enhanced
  %(prog)s --mode generation --platform ios --categories ui
  %(prog)s --enhanced --show-diff --platform web --focus security,testing
        """,
    )
    parser.add_argument(
        "--mode",
        choices=["detection", "generation", "both"],
        default="detection",
        help="Evaluation mode",
    )
    parser.add_argument(
        "--principles", nargs="*", help="Specific principles to test (detection mode)"
    )
    parser.add_argument(
        "--categories", nargs="*", help="Specific categories to test (generation mode)"
    )
    parser.add_argument("--output", help="Output file for report")
    parser.add_argument("--prompt-file", help="File containing prompt to test")
    parser.add_argument(
        "--platform",
        choices=["android", "ios", "web"],
        help="Platform to test (overrides prompt metadata)",
    )
    parser.add_argument("--focus", help="Comma-separated focus areas (overrides prompt metadata)")
    parser.add_argument(
        "--enhanced",
        action="store_true",
        help="Enhance prompt with latest security/accessibility practices using LLM",
    )
    parser.add_argument(
        "--show-diff",
        action="store_true",
        help="Show the difference between original and enhanced prompt",
    )

    args = parser.parse_args()

    # Handle comma-separated principles/categories
    if args.principles:
        # Split comma-separated values and flatten
        principles = []
        for p in args.principles:
            principles.extend([x.strip() for x in p.split(",")])
        args.principles = principles

    if args.categories:
        # Split comma-separated values and flatten
        categories = []
        for c in args.categories:
            categories.extend([x.strip() for x in c.split(",")])
        args.categories = categories

    evaluator = PromptEvaluator()

    # Load prompt to test
    if args.prompt_file:
        with open(args.prompt_file) as f:
            test_prompt = f.read()
    else:
        test_prompt = "You are a code reviewer. Find violations of engineering principles."

    # Parse metadata from prompt if available
    metadata = parse_prompt_metadata(test_prompt) if args.prompt_file else {}

    # Determine effective platform and focus (flags override metadata)
    effective_platform = args.platform or metadata.get("platform")
    effective_focus = args.focus or metadata.get("focus")

    # Parse focus areas if provided
    if effective_focus:
        if isinstance(effective_focus, str):
            effective_principles = [x.strip() for x in effective_focus.split(",")]
        else:
            effective_principles = effective_focus
    else:
        effective_principles = args.principles

    print("Evaluating prompt effectiveness...")
    print(f"Mode: {args.mode}")
    if effective_platform:
        print(f"Platform: {effective_platform}")
    if effective_principles:
        print(f"Focus areas: {', '.join(effective_principles)}")

    if args.mode in ["detection", "both"]:
        print("\nRunning detection evaluation...")

        # Set up API evaluator
        api_evaluator = APIEvaluator("openai", "gpt-4o", "https://api.openai.com/v1")
        ai_func = api_evaluator.evaluate_code
        print("Evaluating with OpenAI GPT-4o...")

        # Enhance prompt if requested
        if args.enhanced:
            test_prompt = enhance_prompt_with_llm(test_prompt, api_evaluator, args.show_diff)

        report = evaluator.evaluate_detection_prompt(test_prompt, ai_func, effective_principles)

        report_text = evaluator.generate_report(report, args.output)
        print(report_text)

    if args.mode in ["generation", "both"]:
        print("\nRunning generation evaluation...")
        results = evaluator.evaluate_generation_prompt(
            test_prompt, mock_ai_generator, mock_ai_evaluator, args.categories
        )

        print(f"Generation Results: {results['average_compliance']:.2%} average compliance")


if __name__ == "__main__":
    main()
